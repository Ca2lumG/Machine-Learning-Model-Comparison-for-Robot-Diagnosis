# -*- coding: utf-8 -*-
"""Model Training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a57QQX_b1Mf3A1aA1xN9nDDOMGnY41Z7
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

import numpy as np
from sklearn.model_selection import train_test_split

lp1 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp1.csv')
lp2 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp2.csv')
lp3 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp3.csv')
lp4 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp4.csv')
lp5 = pd.read_csv('/content/drive/MyDrive/Hyunmin Yoo/Data/cleaned_lp5.csv')

dataset = lp1
dataset.head()

X = dataset.drop('class',axis=1)
y = dataset['class']

X.head()

y.head()

len(y)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#https://scikit-learn.org/stable/modules/multiclass.html

#https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV
#log_reg_model = LogisticRegression(max_iter=1000) - 0.8333333333333334
#log_reg_model = LogisticRegression(max_iter=2500) - 0.8333333333333334
#log_reg_model = LogisticRegression(max_iter=500) - 0.7777777777777778
#log_reg_model = LogisticRegression(max_iter=1500) - 0.8333333333333334
#log_reg_model = DecisionTreeClassifier(max_depth=3) - 0.7777777777777778
#log_reg_model = DecisionTreeClassifier(max_depth=5) - 0.7222222222222222
#log_reg_model = DecisionTreeClassifier(max_depth=2) - 0.7777777777777778
#log_reg_model = DecisionTreeClassifier(max_depth=3, min_samples_split=5) - 0.7777777777777778
#log_reg_model = DecisionTreeClassifier(max_depth=3, min_samples_split=10) - 0.7777777777777778
#log_reg_model = DecisionTreeClassifier(max_depth=3, min_samples_split=20) - 0.7222222222222222
#log_reg_model = KNeighborsClassifier(n_neighbors=3) - 0.8888888888888888
#log_reg_model = KNeighborsClassifier(n_neighbors=1) - 0.8333333333333334
#log_reg_model = KNeighborsClassifier(n_neighbors=5) - 0.6111111111111112
#log_reg_model = KNeighborsClassifier(n_neighbors=7) - 0.5555555555555556
#log_reg_model = KNeighborsClassifier(n_neighbors=4) - 0.7222222222222222
#log_reg_model = KNeighborsClassifier(n_neighbors=2) - 0.8888888888888888
#log_reg_model = MLPClassifier(hidden_layer_sizes=(3)) - 0.5555555555555556
#log_reg_model = MLPClassifier(hidden_layer_sizes=(3,4)) - 0.3888888888888889
#log_reg_model = MLPClassifier(hidden_layer_sizes=(3,4,5)) - 0.6666666666666666
#log_reg_model = MLPClassifier(hidden_layer_sizes=(3,4,5,6)) - 0.5555555555555556
#log_reg_model = MLPClassifier(hidden_layer_sizes=(3,4,5,6,7,8,9,10,11,12)) - 0.8333333333333334
#log_reg_model = MLPClassifier(hidden_layer_sizes=(13,13,15)) - 0.7222222222222222
#log_reg_model = MLPClassifier(hidden_layer_sizes=(20,20,20)) - 0.7222222222222222
#log_reg_model = MLPClassifier(hidden_layer_sizes=(30,30,30)) - 0.7777777777777778
#log_reg_model = MLPClassifier(hidden_layer_sizes=(25,25,20,30)) - 0.7777777777777778
#log_reg_model = MLPClassifier(hidden_layer_sizes=(20)) - 0.7777777777777778
#log_reg_model = MLPClassifier(hidden_layer_sizes=(10,10)) - 0.7222222222222222
#log_reg_model = MLPClassifier(hidden_layer_sizes=(10,15)) - 0.5555555555555556
#log_reg_model = MLPClassifier(hidden_layer_sizes=(20,20)) - 0.9444444444444444
#log_reg_model = MLPClassifier(hidden_layer_sizes=(25,25)) - 0.8888888888888888
#log_reg_model = MLPClassifier(hidden_layer_sizes=(22,23)) - 0.9444444444444444
#log_reg_model = MLPClassifier(hidden_layer_sizes=(24,23)) - 0.8888888888888888
#log_reg_model = MLPClassifier(hidden_layer_sizes=(19,19)) - 0.7777777777777778
#log_reg_model = MLPClassifier(hidden_layer_sizes=(22,22)) - 0.7222222222222222
#log_reg_model = MLPClassifier(hidden_layer_sizes=(23,22)) - 0.8333333333333334
#log_reg_model = MLPClassifier(hidden_layer_sizes=(21,22)) - 0.7777777777777778
#log_reg_model = MLPClassifier(hidden_layer_sizes=(5,21,22)) - 0.6111111111111112
#log_reg_model = MLPClassifier(hidden_layer_sizes=(20,25)) - 0.8888888888888888
#log_reg_model = MLPClassifier(hidden_layer_sizes=(20,20, 5)) - 0.8888888888888888
mlp = MLPClassifier()

parameter_space = {
    'hidden_layer_sizes':[(20,20,5),(24,23),(22,23),(20,20),(10,15),(3,4,5),(5,21,22),(20,25),(21,22),(25,25,20,30),(13,13,15),(3,4,5,6,7,8,9,10,11,12),(3,4),(20),(3,4,5,6),(10),(5)],
    'activation':['identity', 'logistic','tanh','relu'],
    'solver': ['lbfgs','sgd','adam'],
    'alpha': [0.0001,0.05, 0.001, 0.01, 0.5],
    'learning_rate': ['constant','adaptive','invscaling'],
    'max_iter': [1,100,200,500,1000]
}

gs = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=2)
gs.fit(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=42)

parameter_space = {
    'n_estimators': [100, 200, 500, 700, 1000], 'max_depth': [None, 10 ,20], 'min_samples_split': [2], 'min_samples_leaf': [1], 'max_features': ['sqrt']
}

rf = GridSearchCV(rf, parameter_space, n_jobs=-1, cv=2)
rf.fit(X_train, y_train)

print('Best parameters found:\n', rf.best_params_)

'''
Best parameters found:
 {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}
 '''

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb = XGBClassifier(random_state=42)

parameter_space = {
  'max_depth': [3, 4, 5],
  'min_child_weight': [1, 3, 5],
  'subsample': [0.7, 0.8, 0.9],
  'colsample_bytree': [0.7, 0.8, 0.9],
  'gamma': [0, 0.1, 0.3],
  'learning_rate': [0.05, 0.1, 0.2],
  'n_estimators': [100, 200, 300],
  'reg_alpha': [0, 0.1, 1],
  'reg_lambda': [1, 2, 5]
}

xgb = GridSearchCV(xgb, parameter_space, n_jobs=-1, cv=2)
xgb.fit(X_train, y_encoded)

print('Best parameters found:\n', xgb.best_params_)

'''
Best parameters found:
 {'colsample_bytree': 0.7, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 100, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.8}
 '''

"""1. initialize a new model

2. define the parameter space(look up documentation, for finding hyperparameters that can be tuned)

3. set up grid search

4. train the grid search model
"""

print('Best parameters found:\n', log_reg_model.best_params_)

means = log_reg_model.cv_results_['mean_test_score']
stds = log_reg_model.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, log_reg_model.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

#log_reg_model = MLPClassifier(hidden_layer_sizes=(22,23), activation='logistic', alpha = 0.0001, learning_rate= 'constant', solver = 'lbfgs', max_iter= 200) - 0.7777777777777778
model = MLPClassifier(activation = 'logistic', alpha= 0.0001, hidden_layer_sizes= (20, 20), learning_rate= 'invscaling', max_iter= 500, solver= 'lbfgs')
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
print(y_pred)
print(y_test)

from sklearn.metrics import accuracy_score
acc = accuracy_score(y_pred, y_test)
print(acc)

"""class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100,), activation='relu', *, solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000)"""

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

rf_model = RandomForestClassifier(
    n_estimators=300,
    max_depth=None,
    n_jobs=-1,
    class_weight="balanced",
    random_state=42,
)
rf_model.fit(X_train, y_train)
rf_pred = rf_model.predict(X_test)
rf_acc = accuracy_score(y_test, rf_pred)
print("RandomForest accuracy:", rf_acc)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb_model = XGBClassifier(
    n_estimators=500,
    learning_rate=0.05,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_lambda=1.0,
    n_jobs=-1,
    random_state=42,
    tree_method="hist",
    eval_metric="logloss",
)

xgb_model.fit(X_train, y_encoded)
xgb_pred = xgb_model.predict(X_test)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
print("XGBoost accuracy:", xgb_acc)

from sklearn.neural_network import MLPClassifier
mlp_model = MLPClassifier(hidden_layer_sizes=(20,20),random_state=42)
mlp_model.fit(X_train, y_train)
y_pred = mlp_model.predict(X_test)
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_pred, y_test)
print(acc)

#0.7777777777777778 - 1
#0.7222222222222222 - 2
#0.7777777777777778 - 3
#0.5 - 4
#0.7222222222222222 - 5
#0.7777777777777778 - 6
#0.6111111111111112 - 7
#0.6666666666666666 - 8
#0.8333333333333334 - 9
#0.7222222222222222 - 10
#0.6666666666666666 - 11
#0.6666666666666666 - 12
#0.6111111111111112 - 13
#0.7777777777777778 - 14
#0.5555555555555556 - 15
#0.6111111111111112 - 16
#0.6666666666666666 - 17
#0.8888888888888888 - 18
#0.7222222222222222 - 19
#0.5555555555555556 - 20

"""1. Look up why the accuracy is changing each time i train my model. Record 20 times

2. https://scikit-learn.org/stable/modules/cross_validation.html

3. implement random forest and xgbboost

9/11

1. do the grid search for random forest and xgb

2. investigate/ search why accuracy is changing each time + site sources

3. cross validation

4. start filling out final paper doc

5. transform to more formal writing(not first person, third person)

9/21

1. Train the model using the outcome of grid search
2. Compare accuracy training set vs test set (log the hyperparameters, log the accuracy of the training, test sets
3. If there is overfitting, adjust parameters and train again
4. Do these steps for all of the models
"""

#import the models
#define parameter
#do the grid search
#after doing grid search use what it gave me
#find the accuracy of the grid search
#record results
#adjust the hyperparameters and run it again

"""Xgboost Grid Search"""

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder

parameter_space = {
  'max_depth': [3, 4, 5],
  'min_child_weight': [1, 3, 5],
  'subsample': [0.7, 0.8, 0.9],
  'colsample_bytree': [0.7, 0.8, 0.9],
  'gamma': [0, 0.1, 0.3],
  'learning_rate': [0.05, 0.1, 0.2],
  'n_estimators': [100, 200, 300],
  'reg_alpha': [0, 0.1, 1],
  'reg_lambda': [1, 2, 5]
}

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y_train)

xgb = XGBClassifier(random_state=42)

#xgb = GridSearchCV(xgb, parameter_space, n_jobs=-1, cv=2)
#xgb.fit(X_train, y_encoded)

#print('Best parameters found:\n', xgb.best_params_)

from math import gamma
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
#replace the values with the output of grid search
xgb_model = XGBClassifier(
    gamma=0.1,
    min_child_weight=5,
    n_estimators=300,
    learning_rate=0.1,
    max_depth=1,
    subsample=0.6,
    colsample_bytree=0.5,
    reg_lambda=1,
    reg_alpha=0.5,
    n_jobs=-1,
    random_state=42,
)

xgb_model.fit(X_train, y_encoded)

#Find accuracy of test set
xgb_pred = xgb_model.predict(X_test)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_test, xgb_pred)
print("XGBoost test accuracy:", xgb_acc)

#Find accuracy of train set
xgb_pred = xgb_model.predict(X_train)
xgb_pred = label_encoder.inverse_transform(xgb_pred)
xgb_acc = accuracy_score(y_train, xgb_pred)
print("XGBoost train accuracy:", xgb_acc)
#Record the results
#If mismatch adjust and print it again

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

# Decision Tree Classifier Grid Search
dt = DecisionTreeClassifier(random_state=42)

parameter_space_dt = {
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['gini', 'entropy']
}

dt_grid = GridSearchCV(dt, parameter_space_dt, n_jobs=-1, cv=2)
dt_grid.fit(X_train, y_train)

print('Best parameters found for Decision Tree Classifier:\n', dt_grid.best_params_)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Decision Tree Classifier Runner
dt_model = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_leaf=2, min_samples_split=7, random_state=42)

dt_model.fit(X_train, y_train)

# Find accuracy of test set
dt_pred_test = dt_model.predict(X_test)
dt_acc_test = accuracy_score(y_test, dt_pred_test)
print("Decision Tree Classifier test accuracy:", dt_acc_test)

# Find accuracy of train set
dt_pred_train = dt_model.predict(X_train)
dt_acc_train = accuracy_score(y_train, dt_pred_train)
print("Decision Tree Classifier train accuracy:", dt_acc_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rf = RandomForestClassifier(random_state=42)

parameter_space = {
    'n_estimators': [100, 200, 500, 700, 1000],
    'max_depth': [None, 10 ,20],
    'min_samples_split': [2],
    'min_samples_leaf': [1],
    'max_features': ['sqrt']
}

rf = GridSearchCV(rf, parameter_space, n_jobs=-1, cv=2)
rf.fit(X_train, y_train)

print('Best parameters found:\n', rf.best_params_)

#Random Forest Runner
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_features="log2",
    max_depth=10,
    min_samples_leaf=4,
    min_samples_split=5,
    random_state=42,
)

rf_model.fit(X_train, y_train)

# Find accuracy of test set
rf_pred_test = rf_model.predict(X_test)
rf_acc_test = accuracy_score(y_test, rf_pred_test)
print("RandomForest test accuracy:", rf_acc_test)

# Find accuracy of train set
rf_pred_train = rf_model.predict(X_train)
rf_acc_train = accuracy_score(y_train, rf_pred_train)
print("RandomForest train accuracy:", rf_acc_train)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

# KNeighbors Classifier Grid Search
knn = KNeighborsClassifier()

parameter_space_knn = {
    'n_neighbors': [3, 5, 7, 9],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan']
}

knn_grid = GridSearchCV(knn, parameter_space_knn, n_jobs=-1, cv=2)
knn_grid.fit(X_train, y_train)

print('Best parameters found for KNeighbors Classifier:\n', knn_grid.best_params_)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# KNeighbors Classifier Runner
knn_model = KNeighborsClassifier(metric='euclidean', n_neighbors=5, weights='uniform')

knn_model.fit(X_train, y_train)

# Find accuracy of test set
knn_pred_test = knn_model.predict(X_test)
knn_acc_test = accuracy_score(y_test, knn_pred_test)
print("KNeighbors Classifier test accuracy:", knn_acc_test)

# Find accuracy of train set
knn_pred_train = knn_model.predict(X_train)
knn_acc_train = accuracy_score(y_train, knn_pred_train)
print("KNeighbors Classifier train accuracy:", knn_acc_train)

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import GridSearchCV

# MLPClassifier Grid Search
mlp = MLPClassifier(random_state=42)

parameter_space_mlp = {
    'hidden_layer_sizes': [(20,20,5),(24,23),(22,23),(20,20),(10,15),(3,4,5),(5,21,22),(20,25),(21,22),(25,25,20,30),(13,13,15),(3,4,5,6,7,8,9,10,11,12),(3,4),(20),(3,4,5,6),(10),(5)],
    'activation':['identity', 'logistic','tanh','relu'],
    'solver': ['lbfgs','sgd','adam'],
    'alpha': [0.0001,0.05, 0.001, 0.01, 0.5],
    'learning_rate': ['constant','adaptive','invscaling'],
    'max_iter': [100,200,500,1000] # Removed max_iter=1 as it's too small
}

mlp_grid = GridSearchCV(mlp, parameter_space_mlp, n_jobs=-1, cv=2)
mlp_grid.fit(X_train, y_train)

print('Best parameters found for MLPClassifier:\n', mlp_grid.best_params_)

from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

# MLPClassifier Runner
# Replace the parameters below with the output of the grid search
mlp_model = MLPClassifier(hidden_layer_sizes=(22,23), activation='logistic', alpha = 0.05, learning_rate= 'invscaling', solver = 'adam', max_iter= 1500, random_state=42)

mlp_model.fit(X_train, y_train)

# Find accuracy of test set
mlp_pred_test = mlp_model.predict(X_test)
mlp_acc_test = accuracy_score(y_test, mlp_pred_test)
print("MLPClassifier test accuracy:", mlp_acc_test)

# Find accuracy of train set
mlp_pred_train = mlp_model.predict(X_train)
mlp_acc_train = accuracy_score(y_train, mlp_pred_train)
print("MLPClassifier train accuracy:", mlp_acc_train)

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Logistic Regression Grid Search
lr = LogisticRegression(random_state=42, max_iter=1000)

parameter_space_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

lr_grid = GridSearchCV(lr, parameter_space_lr, n_jobs=-1, cv=2)
lr_grid.fit(X_train, y_train)

print('Best parameters found for Logistic Regression:\n', lr_grid.best_params_)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Logistic Regression Runner
lr_model = LogisticRegression(C=0.001, penalty='l2', solver='newton-cg', random_state=42, max_iter=4000)

lr_model.fit(X_train, y_train)

# Find accuracy of test set
lr_pred_test = lr_model.predict(X_test)
lr_acc_test = accuracy_score(y_test, lr_pred_test)
print("Logistic Regression test accuracy:", lr_acc_test)

# Find accuracy of train set
lr_pred_train = lr_model.predict(X_train)
lr_acc_train = accuracy_score(y_train, lr_pred_train)
print("Logistic Regression train accuracy:", lr_acc_train)